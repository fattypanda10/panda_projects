{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "\n",
    "import io # Input/Output Module\n",
    "import os # OS interfaces\n",
    "import cv2 # OpenCV package\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from urllib import request # module for opening HTTP requests\n",
    "from matplotlib import pyplot as plt # Plotting library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:100%; height:140px\">\n",
    "    <img src=\"https://www.kuleuven.be/internationaal/thinktank/fotos-en-logos/ku-leuven-logo.png/image_preview\" width = 300px, heigh = auto align=left>\n",
    "</div>\n",
    "\n",
    "\n",
    "KUL H02A5a Computer Vision: Group Assignment 1\n",
    "---------------------------------------------------------------\n",
    "Student numbers: <span style=\"color:red\">r0846712, r2, r3, r4, r5</span>.\n",
    "\n",
    "The goal of this assignment is to explore more advanced techniques for constructing features that better describe objects of interest and to perform face recognition using these features. This assignment will be delivered in groups of 5 (either composed by you or randomly assigned by your TA's).\n",
    "\n",
    "In this assignment you are a group of computer vision experts that have been invited to ECCV 2021 to do a tutorial about  \"Feature representations, then and now\". To prepare the tutorial you are asked to participate in a kaggle competition and to release a notebook that can be easily studied by the tutorial participants. Your target audience is: (master) students who want to get a first hands-on introduction to the techniques that you apply.\n",
    "\n",
    "---------------------------------------------------------------\n",
    "This notebook is structured as follows:\n",
    "0. Data loading & Preprocessing\n",
    "1. Feature Representations\n",
    "2. Evaluation Metrics \n",
    "3. Classifiers\n",
    "4. Experiments\n",
    "5. Publishing best results\n",
    "6. Discussion\n",
    "\n",
    "Make sure that your notebook is **self-contained** and **fully documented**. Walk us through all steps of your code. Treat your notebook as a tutorial for students who need to get a first hands-on introduction to the techniques that you apply. Provide strong arguments for the design choices that you made and what insights you got from your experiments. Make use of the *Group assignment* forum/discussion board on Toledo if you have any questions.\n",
    "\n",
    "Fill in your student numbers above and get to it! Good luck! \n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>NOTE:</b> This notebook is just a example/template, feel free to adjust in any way you please! Just keep things organised and document accordingly!\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>NOTE:</b> Clearly indicate the improvements that you make!!! You can for instance use titles like: <i>3.1. Improvement: Non-linear SVM with RBF Kernel.<i>\n",
    "</div>\n",
    "    \n",
    "---------------------------------------------------------------\n",
    "# 0. Data loading & Preprocessing\n",
    "\n",
    "## 0.1. Loading data\n",
    "The training set is many times smaller than the test set and this might strike you as odd, however, this is close to a real world scenario where your system might be put through daily use! In this session we will try to do the best we can with the data that we've got! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data from Kaggle and unzip it\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "from zipfile import ZipFile\n",
    "import os\n",
    "\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "competition_name = \"kul-h02a5a-computervision-groupassignment0\"\n",
    "api.competition_download_files(competition_name,\n",
    "                               path=os.getcwd())\n",
    "\n",
    "dataset_name = competition_name + '.zip'\n",
    "with ZipFile(dataset_name, 'r') as zo:\n",
    "    zo.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the relevant data for further use (This cell and the one above replace the original cell for reading the data\n",
    "def readData(filename:str, indexCol:int) -> pd.DataFrame:\n",
    "    df = pd.read_csv(filename, index_col=indexCol)\n",
    "    df.index = df.index.rename('id')\n",
    "    return df\n",
    "\n",
    "train = readData('train_set.csv', 0)\n",
    "test = readData('test_set.csv', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the images (cv2.COLOR_BGR2RGB = 4)\n",
    "import os\n",
    "\n",
    "# Method 1 for getting the images - using a function\n",
    "def readImages(data:str, cv_colorspace:int) -> list:\n",
    "    path = os.path.join(os.getcwd(), data, data)\n",
    "    images = [cv2.cvtColor(np.load(os.path.join(path, (data + '_' + str(i) + '.npy'))), cv_colorspace) for i in eval('range(eval(data).index.size)')]\n",
    "    return np.asarray(images, dtype='object')\n",
    "\n",
    "tr = readImages('train', 4)\n",
    "te = readImages('test', 4)\n",
    "\n",
    "# Method 2 for getting the images - without a function\n",
    "# tr = [cv2.cvtColor(np.load('./train/train/train_{}.npy'.format(index), allow_pickle=False), cv2.COLOR_BGR2RGB) for index, row in train.iterrows()]\n",
    "\n",
    "# te = [cv2.cvtColor(np.load('./test/test/test_{}.npy'.format(index), allow_pickle=False), cv2.COLOR_BGR2RGB) for index, row in test.iterrows()]\n",
    "                \n",
    "print(f\"The training set contains {len(train)} examples, the test set contains {len(test)} examples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the extracted images as a new column to the existing train and test dataframes\n",
    "# (some issues with chained indexing exist)\n",
    "\n",
    "# Method 1 - Using insert\n",
    "# train.insert(1, 'img', tr, True)\n",
    "# test.insert(2, 'img', tr, True)\n",
    "\n",
    "# Method 2 - using loc\n",
    "train.loc[:, 'img'] = tr\n",
    "test.loc[:, 'img'] = te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Input data files are available in the read-only \"../input/\" directory\n",
    "\n",
    "# train = pd.read_csv(\n",
    "#     '/kaggle/input/kul-h02a5a-computervision-groupassignment0/train_set.csv', index_col = 0)\n",
    "# train.index = train.index.rename('id')\n",
    "\n",
    "# test = pd.read_csv(\n",
    "#     '/kaggle/input/kul-h02a5a-computervision-groupassignment0/test_set.csv', index_col = 0)\n",
    "# test.index = test.index.rename('id')\n",
    "\n",
    "# # read the images as numpy arrays and store in \"img\" column\n",
    "# train['img'] = [cv2.cvtColor(np.load('/kaggle/input/kul-h02a5a-computervision-groupassignment0/train/train/train_{}.npy'.format(index), allow_pickle=False), cv2.COLOR_BGR2RGB) \n",
    "#                 for index, row in train.iterrows()]\n",
    "\n",
    "# test['img'] = [cv2.cvtColor(np.load('/kaggle/input/kul-h02a5a-computervision-groupassignment0/test/test/test_{}.npy'.format(index), allow_pickle=False), cv2.COLOR_BGR2RGB) \n",
    "#                 for index, row in test.iterrows()]\n",
    "  \n",
    "\n",
    "# train_size, test_size = len(train),len(test)\n",
    "\n",
    "# \"The training set contains {} examples, the test set contains {} examples.\".format(train_size, test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: this dataset is a subset of the* [*VGG face dataset*](https://www.robots.ox.ac.uk/~vgg/data/vgg_face/).\n",
    "\n",
    "## 0.2. A first look\n",
    "Let's have a look at the data columns and class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The training set contains an identifier, name, image information and class label\n",
    "train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The test set only contains an identifier and corresponding image information.\n",
    "test.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The class distribution in the training set:\n",
    "train.groupby('name').agg({'img':'count', 'class': 'max'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that **Jesse is assigned the classification label 1**, and **Mila is assigned the classification label 2**. The dataset also contains 20 images of **look alikes (assigned classification label 0)** and the raw images. \n",
    "\n",
    "## 0.3. Preprocess data\n",
    "### 0.3.1 Example: HAAR face detector\n",
    "In this example we use the [HAAR feature based cascade classifiers](https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_objdetect/py_face_detection/py_face_detection.html) to detect faces, then the faces are resized so that they all have the same shape. If there are multiple faces in an image, we only take the first one. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b>NOTE:</b> You can write temporary files to <code>/kaggle/temp/</code> or <code>../../tmp</code>, but they won't be saved outside of the current session\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HAARPreprocessor():\n",
    "    \"\"\"Preprocessing pipeline built around HAAR feature based cascade classifiers. \"\"\"\n",
    "    \n",
    "    def __init__(self, path, face_size):\n",
    "        self.face_size = face_size\n",
    "        file_path = os.path.join(path, \"haarcascade_frontalface_default.xml\")\n",
    "        if not os.path.exists(file_path): \n",
    "            if not os.path.exists(path):\n",
    "                os.mkdir(path)\n",
    "            self.download_model(file_path)\n",
    "        \n",
    "        self.classifier = cv2.CascadeClassifier(file_path)\n",
    "  \n",
    "    def download_model(self, path):\n",
    "        url = \"https://raw.githubusercontent.com/opencv/opencv/master/data/\"\\\n",
    "            \"haarcascades/haarcascade_frontalface_default.xml\"\n",
    "        \n",
    "        with request.urlopen(url) as r, open(path, 'wb') as f:\n",
    "            f.write(r.read())\n",
    "            \n",
    "    def detect_faces(self, img):\n",
    "        \"\"\"Detect all faces in an image.\"\"\"\n",
    "        \n",
    "        img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        return self.classifier.detectMultiScale(\n",
    "            img_gray,\n",
    "            scaleFactor=1.2,\n",
    "            minNeighbors=5,\n",
    "            minSize=(30, 30),\n",
    "            flags=cv2.CASCADE_SCALE_IMAGE\n",
    "        )\n",
    "        \n",
    "    def extract_faces(self, img):\n",
    "        \"\"\"Returns all faces (cropped) in an image.\"\"\"\n",
    "        \n",
    "        faces = self.detect_faces(img)\n",
    "\n",
    "        return [img[y:y+h, x:x+w] for (x, y, w, h) in faces]\n",
    "    \n",
    "    def preprocess(self, data_row):\n",
    "        faces = self.extract_faces(data_row['img'])\n",
    "        \n",
    "        # if no faces were found, return None\n",
    "        if len(faces) == 0:\n",
    "            nan_img = np.empty(self.face_size + (3,))\n",
    "            nan_img[:] = np.nan\n",
    "            return nan_img\n",
    "        \n",
    "        # only return the first face\n",
    "        return cv2.resize(faces[0], self.face_size, interpolation = cv2.INTER_AREA)\n",
    "            \n",
    "    def __call__(self, data):\n",
    "        return np.stack([self.preprocess(row) for _, row in data.iterrows()]).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualise**\n",
    "\n",
    "Let's plot a few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter to play with \n",
    "\n",
    "# Rajat - Added data type to imshow\n",
    "FACE_SIZE = (100, 100)\n",
    "\n",
    "def plot_image_sequence(data, n, imgs_per_row=7):\n",
    "    n_rows = 1 + int(n/(imgs_per_row+1))\n",
    "    n_cols = min(imgs_per_row, n)\n",
    "\n",
    "    f,ax = plt.subplots(n_rows,n_cols, figsize=(10*n_cols,10*n_rows))\n",
    "    for i in range(n):\n",
    "        if n == 1:\n",
    "            ax.imshow(data[i].astype('uint8'))\n",
    "        elif n_rows > 1:\n",
    "            ax[int(i/imgs_per_row),int(i%imgs_per_row)].imshow(data[i].astype('uint8'))\n",
    "        else:\n",
    "            ax[int(i%n)].imshow(data[i].astype('uint8'))\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "#preprocessed data \n",
    "preprocessor = HAARPreprocessor(path = '../../tmp', face_size=FACE_SIZE)\n",
    "\n",
    "train_X, train_y = preprocessor(train), train['class'].values\n",
    "test_X = preprocessor(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot faces of Michael and Sarah\n",
    "plot_image_sequence(train_X[train_y == 0], n=20, imgs_per_row=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot faces of Jesse\n",
    "plot_image_sequence(train_X[train_y == 1], n=30, imgs_per_row=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot faces of Mila\n",
    "plot_image_sequence(train_X[train_y == 2], n=30, imgs_per_row=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.4. Store Preprocessed data (optional)\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>NOTE:</b> You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\". Feel free to use this to store intermediary results.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save preprocessed data\n",
    "# prep_path = '/kaggle/working/prepped_data/'\n",
    "# if not os.path.exists(prep_path):\n",
    "#     os.mkdir(prep_path)\n",
    "    \n",
    "# np.save(os.path.join(prep_path, 'train_X.npy'), train_X)\n",
    "# np.save(os.path.join(prep_path, 'train_y.npy'), train_y)\n",
    "# np.save(os.path.join(prep_path, 'test_X.npy'), test_X)\n",
    "\n",
    "# load preprocessed data\n",
    "# prep_path = '/kaggle/working/prepped_data/'\n",
    "# if not os.path.exists(prep_path):\n",
    "#     os.mkdir(prep_path)\n",
    "# train_X = np.load(os.path.join(prep_path, 'train_X.npy'))\n",
    "# train_y = np.load(os.path.join(prep_path, 'train_y.npy'))\n",
    "# test_X = np.load(os.path.join(prep_path, 'test_X.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rajat - saving of data on machine\n",
    "# save preprocessed data\n",
    "CURRENT_PATH = os.getcwd()\n",
    "DATASTORE_PATH = os.path.join(CURRENT_PATH, 'datastore')\n",
    "if not os.path.exists(DATASTORE_PATH):\n",
    "    os.mkdir(DATASTORE_PATH)\n",
    "    \n",
    "np.save(os.path.join(DATASTORE_PATH, 'train_X.npy'), train_X)\n",
    "np.save(os.path.join(DATASTORE_PATH, 'train_y.npy'), train_y)\n",
    "np.save(os.path.join(DATASTORE_PATH, 'test_X.npy'), test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "# load preprocessed data\n",
    "CURRENT_PATH = os.getcwd()\n",
    "DATASTORE_PATH = os.path.join(CURRENT_PATH, 'datastore')\n",
    "train_X = np.load(os.path.join(DATASTORE_PATH, 'train_X.npy'))\n",
    "train_y = np.load(os.path.join(DATASTORE_PATH, 'train_y.npy'))\n",
    "test_X = np.load(os.path.join(DATASTORE_PATH, 'test_X.npy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to rock!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Feature Representations\n",
    "## 1.0. Example: Identify feature extractor\n",
    "Our example feature extractor doesn't actually do anything... It just returns the input:\n",
    "$$\n",
    "\\forall x : f(x) = x.\n",
    "$$\n",
    "\n",
    "It does make for a good placeholder and baseclass ;)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdentityFeatureExtractor:\n",
    "    \"\"\"A simple function that returns the input\"\"\"\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X\n",
    "    \n",
    "    def __call__(self, X):\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Baseline 1: HOG feature extractor/Scale Invariant Feature Transform\n",
    "\n",
    "_**This section deals with the HOG Feature Descriptor. The following are the topics that have been coded and described below. For an in-depth understanding of this topic, sky is the limit. Therefore, only a brief description has been given here. For more specific details, please refer to that particular code where appropriate comments have been given**_ \n",
    "\n",
    "\n",
    "* Features\n",
    "    * Feature Selection\n",
    "    * Feature Detection\n",
    "    * Feature Extraction\n",
    "    * Feature Description\n",
    "* HOG Feature Descriptor | Hog Feature Extractor\n",
    "* Scale Invariant Feature Transform (SIFT)\n",
    "* Feature Matching\n",
    "\n",
    "*********************************************************************************\n",
    "\n",
    "### Features\n",
    "\n",
    "In the context of images, features are essentially low-level characteristics, such as corners, edges, blobs that can be separated out from the images for further processing. The algorithm or the machine learning technique employed can be trained to detect and obtain them automatically. This four steps are generally performed in the same chronological order as they are described here.\n",
    "\n",
    "Feature selection is the process that focusses on choosing the relevant details in images that are required for further training, prediction. In other words, they make up the areas (2D) or the regions (3D) of interest.\n",
    "\n",
    "Feature detection is the technique of finding or locating those required features in the image. There are several algorithms that are related to object detection etc. Thus, the primary objective here is to identify those areas and regions of interest in the images.\n",
    "\n",
    "In order to ensure that the features detected can be used for further image processing and machine learning, it is necessary to convert the extract the detected features and store them in a numerical format that makes it possible to perform mathematical operations on them. \n",
    "\n",
    "Feature description (at times used with feature extraction) is the basically the same thing as feature extraction. It comprises of the feature vectors, called as feature descriptors, which represent the features from  the image in a mathematical form.\n",
    "\n",
    "\n",
    "### Histogram of Oriented Gradients (HOG Feature Extractor/Descriptor) (HOG)\n",
    "\n",
    "On the basis of the above description regarding the features, HOG is a feature descriptor and/or feature extractor that is widely used for computer vision related tasks and image processing that uses the image gradients (magnitude and direction (orientation). It is primarily employed for detecting objects in the images.\n",
    "\n",
    "How it achieves this goal can be understood through the underlying fundamental concept that the local features within an image can be illustrated and described through the variation or the distribution of the intensity gradients. Working with not only the magnitude but also the direction makes HOG quite a nice option as a feature descriptor. It works by segmenting the whole image into a number of rectangular cells (a grid) and then for the pixels contained within each cell, a histogram of gradient directions is calculated. The final descriptor is the combination of all such histograms.\n",
    "\n",
    "HOG descriptor offers some advantages over other descriptors because of the fact that it operates on cells locally and thus is invariant to geometric and photometric transformations other than for the object (in the image to be detected) orientation itself. Thus, in a way, HOG lays emphasis on the structure and object shape. Broadly speaking, the basic steps mentioned below are involved in the calculation of the HOG. This is what happens in the background when the _hog_ function from _sklearn_ is called. Steps like normalization of the block are specified as parameters in the function call\n",
    "\n",
    " * Computation of the Image Gradients (in different directions)\n",
    " * Orientation Bins: creation of the histograms for each cell)\n",
    " * Descriptor Blocks: consists of the local normalization of the gradient strengths\n",
    " * Block Normalization: Four types (L1-sqrt, L1-norm, L2-hys, L2-norm)\n",
    " * Detection of the object\n",
    " \n",
    "As with most of the computer vision tasks, prior to the calculation of the image gradients in X and Y directions (magnitude and direction) for HOG feature extraction, adequate image pre-processing must be performed. This involves re-sizing of the image etc. \n",
    "\n",
    "\n",
    "*********************************************************************************\n",
    "\n",
    "### Scale Invarient Feature Transform (SIFT) and Feature Matching (Brute Force)\n",
    "\n",
    "Similar to HOG, SIFT is too a feature detection algorithm in computer vision, used for detecting and describing the local features in an image. It has many applications with one of the them being object recognition. Firstly, it extracts the keypoints (another name for _features_ and thereafter computes their descriptors. As the name says, a big advantage of this algorithm is its ability to be scale invariant which was an imporvement over other feature extractors/descriptors such as the Harris corner detector which were rotation invariant but not scale invariant. SIFT addresses that problem. \n",
    "\n",
    "There are mainly four parts to implementing SIFT (using OpenCV):\n",
    "\n",
    " * Scale-space Extreme Detection\n",
    " * Localization of the keypoints\n",
    " * Orientation Assignment\n",
    " * Keypoint Descriptor\n",
    " * Matching of the Keypoints\n",
    " \n",
    "After the application of SIFT, a brute force matcher can be used which basically takes the descriptor of an image and matches it with the descriptors in the other image. THis matching is in terms of the distance, Euclidean, for example.\n",
    "\n",
    "Both of these algorithms were coded using openCV and comments at appropriate places in the relevant section below explain more specifically. For more info regarding SIFT, the reader is advised to refer to the original paper by D.Lowe, which talks about SIFT in more detail (_Distinctive Image Features from Scale-Invariant Keypoints_).\n",
    "\n",
    "Also the t-SNE plot which visualises data in higher dimensions in a lower dimension (2 or 3) using a coordinate (location) for each datapoint using a statistical method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class HOGFeatureExtractor(IdentityFeatureExtractor):\n",
    "#     \"\"\"TODO: this feature extractor is under construction\"\"\"\n",
    "    \n",
    "#     def __init__(**params):\n",
    "#         self.params = params\n",
    "        \n",
    "#     def transform(self, X):\n",
    "#         raise NotImplmentedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HOG Feature Extractor\n",
    "from skimage.feature import hog\n",
    "from skimage.transform import resize\n",
    "\n",
    "# Function for finding the HOG Features/Descriptors\n",
    "def hogFeature(image):\n",
    "    fd, hogImage = hog(image, orientations=9, \n",
    "                       pixels_per_cell=(8, 8), \n",
    "                       cells_per_block=(2, 2), \n",
    "                       visualize=True, \n",
    "                       multichannel=True,\n",
    "                       block_norm='L2')\n",
    "    \n",
    "    return fd, hogImage\n",
    "\n",
    "# Calling the HOG Function from above on the Training Dataset\n",
    "# fds consists of the Feature Descriptors for all the images in the Training Dataset\n",
    "# hogs comprises of the HOG images for all the images in the Training Dataset\n",
    "\n",
    "fds, hogs = [], []\n",
    "for i in range(len(train_X)):\n",
    "    feature, hogImage = hogFeature(train_X[i])\n",
    "    fds.append(feature)\n",
    "    hogs.append(hogImage)\n",
    "    \n",
    "# Converstion to Numpy Arrays\n",
    "fds = np.asarray(fds)\n",
    "hogs = np.asarray(hogs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIFT Algorithm (Scale Invariant Feature Transform) - Using OpenCV\n",
    "\n",
    "# Function for finding the Keypoints and the Descriptors using SIFT\n",
    "def siftDescriptor(image):\n",
    "    image = cv2.normalize(image, None, 0, 255, cv2.NORM_MINMAX).astype('uint8')\n",
    "    sift = cv2.SIFT_create()\n",
    "    kp, des = sift.detectAndCompute(image,None)\n",
    "    return kp, des\n",
    "\n",
    "# For plotting the Keypoints\n",
    "def drawkp(image, keypoints):\n",
    "    img = cv2.drawKeypoints(np.uint8(image),\n",
    "                     keypoints,\n",
    "                     None,\n",
    "                     flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "    return img\n",
    "\n",
    "# Using the above two functions on the training dataset\n",
    "kps, descs, kpimages = [], [], []\n",
    "for i in range(len(train_X)):\n",
    "    keypt, desc = siftDescriptor(train_X[i])\n",
    "    kps.append(keypt)\n",
    "    descs.append(desc)\n",
    "    img = drawkp(train_X[i], keypt)\n",
    "    kpimages.append(img)\n",
    "    \n",
    "# Conversion to Numpy Arrays\n",
    "kps = np.asarray(kps, dtype='object')\n",
    "descs = np.asarray(descs)\n",
    "kpimages = np.asarray(kpimages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matching (using opencv) - Brute Force Matching (for SIFT based Descriptor)\n",
    "\n",
    "# Function for the Brute Force Feature Matching using OpenCV\n",
    "def bruteMatch(test_image, test_against_desc):\n",
    "    matches, good = 0, []\n",
    "    sift = cv2.SIFT_create()\n",
    "    kpTest, desTest = siftDescriptor(test_image)\n",
    "    bf = cv2.BFMatcher()\n",
    "    if (desTest is not None) and (test_against_desc is not None):\n",
    "        matches = bf.knnMatch(desTest, test_against_desc, k=2)\n",
    "        for m, n in matches:\n",
    "            if m.distance < 0.75*n.distance:\n",
    "                good.append([m])\n",
    "    return matches, good, kpTest\n",
    "\n",
    "def bruteDraw(test_image, test_against_image, kp1, kp2, good):\n",
    "    img = cv2.drawMatchesKnn(np.uint8(test_image),\n",
    "                              kp1,\n",
    "                              np.uint8(test_against_image),\n",
    "                              kp2,\n",
    "                              good,\n",
    "                              None,\n",
    "                              flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Maatching using Brute Force Technique (OpenCV) - Best Match\n",
    "\n",
    "# Test Image that needs to be Matched *(User-Input)*\n",
    "test_image = test_X[25]\n",
    "\n",
    "# Finding the best match - Scanning through the Training Dataset for the specifc test image\n",
    "most_goods_len, most_goods_image_index, most_goods_matches, most_goods  = 0, 0, 0, 0\n",
    "for i in range(len(train_X)):\n",
    "    match, goods, kpTest = bruteMatch(test_image, descs[i])\n",
    "    if match != 0 or len(goods) != 0 or kpTest != 0:\n",
    "        if (len(goods) > most_goods_len):\n",
    "            most_goods_len = len(goods)\n",
    "            most_goods_image_index= i\n",
    "            most_goods_matches = match\n",
    "            most_goods = goods\n",
    "\n",
    "# Feature Matching using the previously defined functions\n",
    "best_image_match = bruteDraw(test_image, \n",
    "                             train_X[most_goods_image_index], \n",
    "                             kpTest, \n",
    "                             kps[most_goods_image_index], \n",
    "                             most_goods)\n",
    "\n",
    "# Plotting the Best Matches \n",
    "if best_image_match is not None:\n",
    "    person = ''\n",
    "    classIndex = train.loc[most_goods_image_index, 'class']\n",
    "    plt.imshow(best_image_match)\n",
    "    plt.title(\"Feature Matching: Test Image (Left) | Best Match (Right)\")\n",
    "    if classIndex == 1:\n",
    "        person = 'Jesse'\n",
    "    elif classIndex == 2:\n",
    "        person = 'Mila'\n",
    "    else:\n",
    "        person = 'Other'\n",
    "\n",
    "    print(f\"Closest Match --> Class: {classIndex}, Name: {person}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next steps\n",
    "# code the HOG feature extractor for the train_X images, get the features\n",
    "# do a matching using the feature matching also given in opencv and also check skimage\n",
    "# then implement SIFT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1. t-SNE Plots\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T-SNE Calculation using HOG FD\n",
    "from sklearn.manifold import TSNE  \n",
    "tsne = TSNE(n_components=2).fit_transform(fds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE Plotting\n",
    "\n",
    "# Scaling from 0 to 1 and starting from 0\n",
    "def scaling(data):\n",
    "    val_range = (np.max(data) - np.min(data))\n",
    "    start0 = data - np.min(data)\n",
    "    scaled = start0 / val_range \n",
    "    return scaled\n",
    "\n",
    "# Scaling and Plotting the t-SNE\n",
    "tx = tsne[:, 0]\n",
    "ty = tsne[:, 1]\n",
    "tx = scaling(tx)\n",
    "ty = scaling(ty)\n",
    "y_data = np.asarray(train.loc[:, 'class'])\n",
    "plt.scatter(tx, ty, c=y_data, cmap=plt.cm.get_cmap(\"jet\", 3))\n",
    "plt.colorbar(ticks=range(3))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2. Discussion\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Baseline 2: PCA feature extractor\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCAFeatureExtractor(IdentityFeatureExtractor):\n",
    "    \"\"\"TODO: this feature extractor is under construction\"\"\"\n",
    "    \n",
    "    def __init__(self, n_components):\n",
    "        self.n_components = n_components\n",
    "        \n",
    "    def transform(self, X):\n",
    "        raise NotImplmentedError\n",
    "        \n",
    "    def inverse_transform(self, X):\n",
    "        raise NotImplmentedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1. Eigenface Plots\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2. Feature Space Plots\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3. Discussion\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Evaluation Metrics\n",
    "## 2.0. Example: Accuracy\n",
    "As example metric we take the accuracy. Informally, accuracy is the proportion of correct predictions over the total amount of predictions. It is used a lot in classification but it certainly has its disadvantages..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Classifiers\n",
    "## 3.0. Example: The *'not so smart'* classifier\n",
    "This random classifier is not very complicated. It makes predictions at random, based on the distribution obseved in the training set. **It thus assumes** that the class labels of the test set will be distributed similarly to the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomClassificationModel:\n",
    "    \"\"\"Random classifier, draws a random sample based on class distribution observed \n",
    "    during training.\"\"\"\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Adjusts the class ratio instance variable to the one observed in y. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : tensor\n",
    "            Training set\n",
    "        y : array\n",
    "            Training set labels\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : RandomClassificationModel\n",
    "        \"\"\"\n",
    "        \n",
    "        self.classes, self.class_ratio = np.unique(y, return_counts=True)\n",
    "        self.class_ratio = self.class_ratio / self.class_ratio.sum()\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"Samples labels for the input data. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : tensor\n",
    "            dataset\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        y_star : array\n",
    "            'Predicted' labels\n",
    "        \"\"\"\n",
    "\n",
    "        np.random.seed(0)\n",
    "        return np.random.choice(self.classes, size = X.shape[0], p=self.class_ratio)\n",
    "    \n",
    "    def __call__(self, X):\n",
    "        return self.predict(X)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Baseline 1: My favorite classifier\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FavoriteClassificationModel:\n",
    "    \"\"\"TODO: this classifier is under construction.\"\"\"\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        raise NotImplmentedError\n",
    "        \n",
    "    def predict(self, X):\n",
    "        raise NotImplmentedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Experiments\n",
    "<div class=\"alert alert-block alert-info\"> <b>NOTE:</b> Do <i>NOT</i> use this section to keep track of every little change you make in your code! Instead, highlight the most important findings and the major (best) pipelines that you've discovered.  \n",
    "</div>\n",
    "<br>\n",
    "\n",
    "## 4.0. Example: basic pipeline\n",
    "The basic pipeline takes any input and samples a label based on the class label distribution of the training set. As expected the performance is very poor, predicting approximately 1/4 correctly on the training set. There is a lot of room for improvement but this is left to you ;). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = IdentityFeatureExtractor() \n",
    "classifier = RandomClassificationModel()\n",
    "\n",
    "# train the model on the features\n",
    "classifier.fit(feature_extractor(train_X), train_y)\n",
    "\n",
    "# model/final pipeline\n",
    "model = lambda X: classifier(feature_extractor(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate performance of the model on the training set\n",
    "train_y_star = model(train_X)\n",
    "\n",
    "\"The performance on the training set is {:.2f}. This however, does not tell us much about the actual performance (generalisability).\".format(\n",
    "    accuracy_score(train_y, train_y_star))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the labels for the test set \n",
    "test_y_star = model(test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Publishing best results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = test.copy().drop('img', axis = 1)\n",
    "submission['class'] = test_y_star\n",
    "\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Discussion\n",
    "...\n",
    "\n",
    "In summary we contributed the following: \n",
    "* \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
